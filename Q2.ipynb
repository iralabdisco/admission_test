{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Use a framework of your choice (preferably Pytorch) to implement a simple neural network and the corresponding loss\n",
    "function for image classification task. You are only required to implement one class for the network definition and one\n",
    "function for the loss (docstrings are welcome). Main function and training loop are not required.\n",
    "2. Modify the network just created to perform pixel-level semantic segmentation.\n",
    "3. Assuming a batch size B and input images shape (H, W, 3), what is the shape of the input tensor of the two networks you\n",
    "implemented? and the corresponding output tensors shape?\n",
    "4. Given a network with an image as input and an embedding vector as output, and another network with a word as input\n",
    "and an embedding vector as output, how would you train those two networks so that images and words representing the\n",
    "same concept would have similar embedding vectors? [open-ended question]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# answer to 4-1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# answer to 4-1\n",
    "### Net1 : image -> Model -> vector1\n",
    "### Net2:  word -> Model - > vector2\n",
    "\n",
    "Honestly this question first remind me the following paper:\n",
    "\"See, Hear, and Read: Deep Aligned Representations\"\n",
    "However, in this question it seems we can not combine these two embeddings something like image captioning approach or the idea in this paper. \n",
    "\n",
    "After some research, I found this paper: \n",
    "\"Learning Deep Structure-Preserving Image-Text Embeddings\n",
    "\" : http://slazebni.cs.illinois.edu/publications/cvpr16_structure.pdf\n",
    "And the idea of Bi-directional ranking constraints in this paper gave me some hints to develope the idea. \n",
    "\n",
    "Given a training image x1, training word x2, let Y+ and Yâˆ’ denote its sets of matching (positive) and non-matching (negative) concepts, respectively.\n",
    "We want the distance between x1 and x2 embedding vector with positive concepts y+ to be smaller than the distance between x1 and x2 embedding vector with negative concept y- by some enforced margin m.\n",
    "\n",
    "dP(x1Em, x2Em) + m < dN(x1Em, x2Em)\n",
    "\n",
    "x1Em : Output of the first neural network\n",
    "x2Em : Output of the second neural network\n",
    "dP: Distance for samples with positive concepts\n",
    "dN: Distance for samples with negative concepts\n",
    "\n",
    "This is the constraint that I have to convert to an equation. \n",
    "\n",
    "In the following equations for simplicity I will define Y. Y is 1 if the samples are in the set of y+ and Y is 0 if the samples are in the set of y- or negative concepts. \n",
    "\n",
    "So in order to define a loss function I will use Hinge/Contrastive Loss as following:\n",
    "L(x1,x2) = Y *0.5* {dP}^2 + (1-Y)*0.5* {max(0,m-dN)}^2\n",
    "\n",
    "As you can see, in this loss funciton we will decrease the distance between two embeddings for possitive concepts and we will increase the distance between two embeddings for negative concepts. \n",
    "\n",
    "The only problem is providing the dataset. I think, if we want to train networks like that we have to define a training dataset with positive and negative samples similar to Dr. LIM approach (\"Dimensionality Reduction by Learning an Invariant Mapping\n",
    "\"). \n",
    "\n",
    "Please notice that, the whole idea is very similar to the siamese architectures. \n",
    "\n",
    "Later we can also use techniques like Structure-preserving as mentioned in that paper to make sure the embedding for similar samples are also similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a simple nn : image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# answer to 3-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer to 3-1 : Consider H and W equal 32 and you can find shape of the input and output tensor in the next blocks for two networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# answer to 2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer to 2-1\n",
    "class CNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.inputShape = [32,32,3]\n",
    "        self.classNumber = 10 # number of class in the data-set\n",
    "        self.x = x # features\n",
    "        self.y = y # ground truth\n",
    "        \n",
    "    # x is input\n",
    "    def arch(self):\n",
    "        #TODO: add dropout and bn later\n",
    "        self.inputLayer = tf.reshape(self.x,[-1,self.inputLayer]) # b,32,32,3\n",
    "        \n",
    "        # layer 1\n",
    "        layer = tf.layers.conv2d(inputs=self.inputLayer, filters=40, kernel_size=[5,5], padding=\"same\", activation=tf.nn.relu)\n",
    "        # b,32,32,40\n",
    "        layer = tf.layers.max_pooling2d(inputs=layer, pool_size=[2,2], strides=2) # pooling layer\n",
    "        # b,16,16,40\n",
    "        \n",
    "        # layer 2\n",
    "        layer = tf.layers.conv2d(inputs=layer, filters=80, kernel_size=[5,5], padding=\"same\", activation=tf.nn.relu)\n",
    "        # b,16,16,80\n",
    "        layer = tf.layers.max_pooling2d(inputs=layer, pool_size=[2,2], strides=2) # pooling layer\n",
    "        # b,8,8,80\n",
    "        \n",
    "        # dense to classify\n",
    "        layer = tf.reshape(layer, [-1,8*8*80]) # flatten the conv layer \n",
    "        layer = tf.layers.dense(inputs=layer, units=800, activation=tf.nn.relu) # Dense Layer\n",
    "        self.output = tf.layers.dense(inputs=layer, units=self.classNumber) # logits pass to loss funciton\n",
    "        \n",
    "    def loss(self):\n",
    "        self.loss = tf.losses.sparse_softmax_cross_entropy(labels=self.y, logits=self.output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# answer to 2-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer to 2-2\n",
    "# convert the CNN to FCN - Fully Convolutional Neural NEtwork\n",
    "class FCNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.inputShape = [32,32,3]\n",
    "        self.classNumber = 10 # number of class in the data-set\n",
    "        self.x = x # features\n",
    "        self.y = y # ground truth\n",
    "        \n",
    "    # x is input\n",
    "    def arch(self):\n",
    "        #TODO: add dropout, bn, regularization and skip-connection later\n",
    "        self.inputLayer = tf.reshape(self.x,[-1,self.inputLayer]) # b,32,32,3\n",
    "        \n",
    "        # layer 1\n",
    "        layer = tf.layers.conv2d(inputs=self.inputLayer, filters=40, kernel_size=[5,5], padding=\"same\", activation=tf.nn.relu)\n",
    "        # b,32,32,40\n",
    "        layer = tf.layers.max_pooling2d(inputs=layer, pool_size=[2,2], strides=2) # pooling layer\n",
    "        # b,16,16,40\n",
    "        \n",
    "        # layer 2\n",
    "        layer = tf.layers.conv2d(inputs=layer, filters=80, kernel_size=[5,5], padding=\"same\", activation=tf.nn.relu)\n",
    "        # b,16,16,80\n",
    "        layer = tf.layers.max_pooling2d(inputs=layer, pool_size=[2,2], strides=2) # pooling layer\n",
    "        # b,8,8,80\n",
    "        \n",
    "        # conv 1x1 --> same as fully connected, spatial version\n",
    "        layer = tf.layers.conv2d(inputs=layer, filters=self.classNumber, kernel_size=[1,1], padding=\"same\")\n",
    "        # b,8,8,classNumber\n",
    "        \n",
    "        # use the reverse function to build up the image again : upsampling\n",
    "        # layer 2 reverse\n",
    "        layer = tf.layers.conv2d_transpose(inputs=layer, filters=80, strides=[2,2],output_shape= [-1,16,16,80],kernel_size=[5,5], padding=\"same\")\n",
    "        # b,16,16,80\n",
    "        \n",
    "        # layer 1 reverese\n",
    "        layer = tf.layers.conv2d_transpose(inputs=layer, filters=40, strides=[2,2],output_shape= [-1,32,32,40],kernel_size=[5,5], padding=\"same\")\n",
    "        # b,32,32,40\n",
    "        \n",
    "        self.output = tf.layers.conv2d_transpose(inputs=layer, filters=self.classNumber, strides=[1,1],output_shape= [-1,32,32,self.classNumber],kernel_size=[5,5], padding=\"same\")# back to input, the cchannels are the class outputs\n",
    "        # b,32,32,classNumber\n",
    "        \n",
    "    def loss(self):\n",
    "        yResahepd = tf.reshape(self.y,[-1,self.classNumber])\n",
    "        outputReshaped = rf.reshape(self.outputReshaped,[-1,self.classNumber])\n",
    "        self.loss = tf.reduce_mean(tf.losses.softmax_cross_entropy_with_logits(labels=yReshaped, logits=outputReshaped))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
